{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial To Implement k-Nearest Neighbors in Python From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is k-Nearest Neighbors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for kNN is the entire training dataset. When a prediction is required for a unseen data instance, the kNN algorithm will search through the training dataset for the k-most similar instances. The prediction attribute of the most similar instances is summarized and returned as the prediction for the unseen instance.\n",
    "\n",
    "The similarity measure is dependent on the type of data. For real-valued data, the Euclidean distance can be used. Other other types of data such as categorical or binary data, Hamming distance can be used.\n",
    "\n",
    "In the case of regression problems, the average of the predicted attribute may be returned. In the case of classification, the most prevalent class may be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does k-Nearest Neighbors Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN algorithm is belongs to the family of instance-based, lazy learning algorithms.\n",
    "\n",
    "Instance-based algorithms are those algorithms that model the problem using data instances (or rows) in order to make predictive decisions. The kNN algorithm is an extreme form of instance-based methods because all training observations are retained as part of the model.\n",
    "\n",
    "Lazy learning refers to the fact that the algorithm does not build a model until the time that a prediction is required. It is lazy because it only does work at the last second. This has the benefit of only including data relevant to the unseen data, called a localized model. A disadvantage is that it can be computationally expensive to repeat the same or similar searches over larger training datasets.\n",
    "\n",
    "Finally, kNN is powerful because it does not assume anything about the data, other than a distance measure can be calculated consistently between any two instances. As such, it is called non-parametric or non-linear as it does not assume a functional form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Flowers Using Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test problem we will be using in this tutorial is iris classification.\n",
    "\n",
    "The problem is comprised of 150 observations of iris flowers from three different species. There are 4 measurements of given flowers: sepal length, sepal width, petal length and petal width, all in the same unit of centimeters. The predicted attribute is the species, which is one of setosa, versicolor or virginica.\n",
    "\n",
    "It is a standard dataset where the species is known for all instances. As such we can split the data into training and test datasets and use the results to evaluate our algorithm implementation. Good classification accuracy on this problem is above 90% correct, typically 96% or better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handle Data, Load and Split into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1, 3.5, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.0, 1.4, 0.2, Iris-setosa\n",
      "4.7, 3.2, 1.3, 0.2, Iris-setosa\n",
      "4.6, 3.1, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.6, 1.4, 0.2, Iris-setosa\n",
      "5.4, 3.9, 1.7, 0.4, Iris-setosa\n",
      "4.6, 3.4, 1.4, 0.3, Iris-setosa\n",
      "5.0, 3.4, 1.5, 0.2, Iris-setosa\n",
      "4.4, 2.9, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "5.4, 3.7, 1.5, 0.2, Iris-setosa\n",
      "4.8, 3.4, 1.6, 0.2, Iris-setosa\n",
      "4.8, 3.0, 1.4, 0.1, Iris-setosa\n",
      "4.3, 3.0, 1.1, 0.1, Iris-setosa\n",
      "5.8, 4.0, 1.2, 0.2, Iris-setosa\n",
      "5.7, 4.4, 1.5, 0.4, Iris-setosa\n",
      "5.4, 3.9, 1.3, 0.4, Iris-setosa\n",
      "5.1, 3.5, 1.4, 0.3, Iris-setosa\n",
      "5.7, 3.8, 1.7, 0.3, Iris-setosa\n",
      "5.1, 3.8, 1.5, 0.3, Iris-setosa\n",
      "5.4, 3.4, 1.7, 0.2, Iris-setosa\n",
      "5.1, 3.7, 1.5, 0.4, Iris-setosa\n",
      "4.6, 3.6, 1.0, 0.2, Iris-setosa\n",
      "5.1, 3.3, 1.7, 0.5, Iris-setosa\n",
      "4.8, 3.4, 1.9, 0.2, Iris-setosa\n",
      "5.0, 3.0, 1.6, 0.2, Iris-setosa\n",
      "5.0, 3.4, 1.6, 0.4, Iris-setosa\n",
      "5.2, 3.5, 1.5, 0.2, Iris-setosa\n",
      "5.2, 3.4, 1.4, 0.2, Iris-setosa\n",
      "4.7, 3.2, 1.6, 0.2, Iris-setosa\n",
      "4.8, 3.1, 1.6, 0.2, Iris-setosa\n",
      "5.4, 3.4, 1.5, 0.4, Iris-setosa\n",
      "5.2, 4.1, 1.5, 0.1, Iris-setosa\n",
      "5.5, 4.2, 1.4, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "5.0, 3.2, 1.2, 0.2, Iris-setosa\n",
      "5.5, 3.5, 1.3, 0.2, Iris-setosa\n",
      "4.9, 3.1, 1.5, 0.1, Iris-setosa\n",
      "4.4, 3.0, 1.3, 0.2, Iris-setosa\n",
      "5.1, 3.4, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.5, 1.3, 0.3, Iris-setosa\n",
      "4.5, 2.3, 1.3, 0.3, Iris-setosa\n",
      "4.4, 3.2, 1.3, 0.2, Iris-setosa\n",
      "5.0, 3.5, 1.6, 0.6, Iris-setosa\n",
      "5.1, 3.8, 1.9, 0.4, Iris-setosa\n",
      "4.8, 3.0, 1.4, 0.3, Iris-setosa\n",
      "5.1, 3.8, 1.6, 0.2, Iris-setosa\n",
      "4.6, 3.2, 1.4, 0.2, Iris-setosa\n",
      "5.3, 3.7, 1.5, 0.2, Iris-setosa\n",
      "5.0, 3.3, 1.4, 0.2, Iris-setosa\n",
      "7.0, 3.2, 4.7, 1.4, Iris-versicolor\n",
      "6.4, 3.2, 4.5, 1.5, Iris-versicolor\n",
      "6.9, 3.1, 4.9, 1.5, Iris-versicolor\n",
      "5.5, 2.3, 4.0, 1.3, Iris-versicolor\n",
      "6.5, 2.8, 4.6, 1.5, Iris-versicolor\n",
      "5.7, 2.8, 4.5, 1.3, Iris-versicolor\n",
      "6.3, 3.3, 4.7, 1.6, Iris-versicolor\n",
      "4.9, 2.4, 3.3, 1.0, Iris-versicolor\n",
      "6.6, 2.9, 4.6, 1.3, Iris-versicolor\n",
      "5.2, 2.7, 3.9, 1.4, Iris-versicolor\n",
      "5.0, 2.0, 3.5, 1.0, Iris-versicolor\n",
      "5.9, 3.0, 4.2, 1.5, Iris-versicolor\n",
      "6.0, 2.2, 4.0, 1.0, Iris-versicolor\n",
      "6.1, 2.9, 4.7, 1.4, Iris-versicolor\n",
      "5.6, 2.9, 3.6, 1.3, Iris-versicolor\n",
      "6.7, 3.1, 4.4, 1.4, Iris-versicolor\n",
      "5.6, 3.0, 4.5, 1.5, Iris-versicolor\n",
      "5.8, 2.7, 4.1, 1.0, Iris-versicolor\n",
      "6.2, 2.2, 4.5, 1.5, Iris-versicolor\n",
      "5.6, 2.5, 3.9, 1.1, Iris-versicolor\n",
      "5.9, 3.2, 4.8, 1.8, Iris-versicolor\n",
      "6.1, 2.8, 4.0, 1.3, Iris-versicolor\n",
      "6.3, 2.5, 4.9, 1.5, Iris-versicolor\n",
      "6.1, 2.8, 4.7, 1.2, Iris-versicolor\n",
      "6.4, 2.9, 4.3, 1.3, Iris-versicolor\n",
      "6.6, 3.0, 4.4, 1.4, Iris-versicolor\n",
      "6.8, 2.8, 4.8, 1.4, Iris-versicolor\n",
      "6.7, 3.0, 5.0, 1.7, Iris-versicolor\n",
      "6.0, 2.9, 4.5, 1.5, Iris-versicolor\n",
      "5.7, 2.6, 3.5, 1.0, Iris-versicolor\n",
      "5.5, 2.4, 3.8, 1.1, Iris-versicolor\n",
      "5.5, 2.4, 3.7, 1.0, Iris-versicolor\n",
      "5.8, 2.7, 3.9, 1.2, Iris-versicolor\n",
      "6.0, 2.7, 5.1, 1.6, Iris-versicolor\n",
      "5.4, 3.0, 4.5, 1.5, Iris-versicolor\n",
      "6.0, 3.4, 4.5, 1.6, Iris-versicolor\n",
      "6.7, 3.1, 4.7, 1.5, Iris-versicolor\n",
      "6.3, 2.3, 4.4, 1.3, Iris-versicolor\n",
      "5.6, 3.0, 4.1, 1.3, Iris-versicolor\n",
      "5.5, 2.5, 4.0, 1.3, Iris-versicolor\n",
      "5.5, 2.6, 4.4, 1.2, Iris-versicolor\n",
      "6.1, 3.0, 4.6, 1.4, Iris-versicolor\n",
      "5.8, 2.6, 4.0, 1.2, Iris-versicolor\n",
      "5.0, 2.3, 3.3, 1.0, Iris-versicolor\n",
      "5.6, 2.7, 4.2, 1.3, Iris-versicolor\n",
      "5.7, 3.0, 4.2, 1.2, Iris-versicolor\n",
      "5.7, 2.9, 4.2, 1.3, Iris-versicolor\n",
      "6.2, 2.9, 4.3, 1.3, Iris-versicolor\n",
      "5.1, 2.5, 3.0, 1.1, Iris-versicolor\n",
      "5.7, 2.8, 4.1, 1.3, Iris-versicolor\n",
      "6.3, 3.3, 6.0, 2.5, Iris-virginica\n",
      "5.8, 2.7, 5.1, 1.9, Iris-virginica\n",
      "7.1, 3.0, 5.9, 2.1, Iris-virginica\n",
      "6.3, 2.9, 5.6, 1.8, Iris-virginica\n",
      "6.5, 3.0, 5.8, 2.2, Iris-virginica\n",
      "7.6, 3.0, 6.6, 2.1, Iris-virginica\n",
      "4.9, 2.5, 4.5, 1.7, Iris-virginica\n",
      "7.3, 2.9, 6.3, 1.8, Iris-virginica\n",
      "6.7, 2.5, 5.8, 1.8, Iris-virginica\n",
      "7.2, 3.6, 6.1, 2.5, Iris-virginica\n",
      "6.5, 3.2, 5.1, 2.0, Iris-virginica\n",
      "6.4, 2.7, 5.3, 1.9, Iris-virginica\n",
      "6.8, 3.0, 5.5, 2.1, Iris-virginica\n",
      "5.7, 2.5, 5.0, 2.0, Iris-virginica\n",
      "5.8, 2.8, 5.1, 2.4, Iris-virginica\n",
      "6.4, 3.2, 5.3, 2.3, Iris-virginica\n",
      "6.5, 3.0, 5.5, 1.8, Iris-virginica\n",
      "7.7, 3.8, 6.7, 2.2, Iris-virginica\n",
      "7.7, 2.6, 6.9, 2.3, Iris-virginica\n",
      "6.0, 2.2, 5.0, 1.5, Iris-virginica\n",
      "6.9, 3.2, 5.7, 2.3, Iris-virginica\n",
      "5.6, 2.8, 4.9, 2.0, Iris-virginica\n",
      "7.7, 2.8, 6.7, 2.0, Iris-virginica\n",
      "6.3, 2.7, 4.9, 1.8, Iris-virginica\n",
      "6.7, 3.3, 5.7, 2.1, Iris-virginica\n",
      "7.2, 3.2, 6.0, 1.8, Iris-virginica\n",
      "6.2, 2.8, 4.8, 1.8, Iris-virginica\n",
      "6.1, 3.0, 4.9, 1.8, Iris-virginica\n",
      "6.4, 2.8, 5.6, 2.1, Iris-virginica\n",
      "7.2, 3.0, 5.8, 1.6, Iris-virginica\n",
      "7.4, 2.8, 6.1, 1.9, Iris-virginica\n",
      "7.9, 3.8, 6.4, 2.0, Iris-virginica\n",
      "6.4, 2.8, 5.6, 2.2, Iris-virginica\n",
      "6.3, 2.8, 5.1, 1.5, Iris-virginica\n",
      "6.1, 2.6, 5.6, 1.4, Iris-virginica\n",
      "7.7, 3.0, 6.1, 2.3, Iris-virginica\n",
      "6.3, 3.4, 5.6, 2.4, Iris-virginica\n",
      "6.4, 3.1, 5.5, 1.8, Iris-virginica\n",
      "6.0, 3.0, 4.8, 1.8, Iris-virginica\n",
      "6.9, 3.1, 5.4, 2.1, Iris-virginica\n",
      "6.7, 3.1, 5.6, 2.4, Iris-virginica\n",
      "6.9, 3.1, 5.1, 2.3, Iris-virginica\n",
      "5.8, 2.7, 5.1, 1.9, Iris-virginica\n",
      "6.8, 3.2, 5.9, 2.3, Iris-virginica\n",
      "6.7, 3.3, 5.7, 2.5, Iris-virginica\n",
      "6.7, 3.0, 5.2, 2.3, Iris-virginica\n",
      "6.3, 2.5, 5.0, 1.9, Iris-virginica\n",
      "6.5, 3.0, 5.2, 2.0, Iris-virginica\n",
      "6.2, 3.4, 5.4, 2.3, Iris-virginica\n",
      "5.9, 3.0, 5.1, 1.8, Iris-virginica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('iris.data', 'r') as csvfile:\n",
    "\tlines = csv.reader(csvfile)\n",
    "\tfor row in lines:\n",
    "\t\tprint(', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "def loadDataset(filename, split, trainingSet=[] , testSet=[]):\n",
    "\twith open(filename, 'r') as csvfile:\n",
    "\t    lines = csv.reader(csvfile)\n",
    "\t    dataset = list(lines)\n",
    "\t    for x in range(len(dataset)-1):\n",
    "\t        for y in range(4):\n",
    "\t            dataset[x][y] = float(dataset[x][y])\n",
    "\t        if random.random() < split:\n",
    "\t            trainingSet.append(dataset[x])\n",
    "\t        else:\n",
    "\t            testSet.append(dataset[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 105\n",
      "Test: 45\n"
     ]
    }
   ],
   "source": [
    "trainingSet=[]\n",
    "testSet=[]\n",
    "loadDataset('iris.data', 0.66, trainingSet, testSet)\n",
    "print('Train: ' + repr(len(trainingSet)))\n",
    "print('Test: ' + repr(len(testSet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions we need to calculate the similarity between any two given data instances. This is needed so that we can locate the k most similar data instances in the training dataset for a given member of the test dataset and in turn make a prediction.\n",
    "\n",
    "Given that all four flower measurements are numeric and have the same units, we can directly use the Euclidean distance measure. This is defined as the square root of the sum of the squared differences between the two arrays of numbers (read that again a few times and let it sink in).\n",
    "\n",
    "Additionally, we want to control which fields to include in the distance calculation. Specifically, we only want to include the first 4 attributes. One approach is to limit the euclidean distance to a fixed length, ignoring the final dimension.\n",
    "\n",
    "Putting all of this together we can define the euclideanDistance function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "\tdistance = 0\n",
    "\tfor x in range(length):\n",
    "\t\tdistance += pow((instance1[x] - instance2[x]), 2)\n",
    "\treturn math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 3.4641016151377544\n"
     ]
    }
   ],
   "source": [
    "data1 = [2, 2, 2, 'a']\n",
    "data2 = [4, 4, 4, 'b']\n",
    "distance = euclideanDistance(data1, data2, 3)\n",
    "print('Distance: ' + repr(distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a similarity measure, we can use it collect the k most similar instances for a given unseen instance.\n",
    "\n",
    "This is a straight forward process of calculating the distance for all instances and selecting a subset with the smallest distance values.\n",
    "\n",
    "Below is the getNeighbors function that returns k most similar neighbors from the training set for a given test instance (using the already defined euclideanDistance function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "\tdistances = []\n",
    "\tlength = len(testInstance)-1\n",
    "\tfor x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "\tdistances.sort(key=operator.itemgetter(1))\n",
    "\tneighbors = []\n",
    "\tfor x in range(k):\n",
    "\t\tneighbors.append(distances[x][0])\n",
    "\treturn neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 4, 4, 'b']]\n"
     ]
    }
   ],
   "source": [
    "trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']]\n",
    "testInstance = [5, 5, 5]\n",
    "k = 1\n",
    "neighbors = getNeighbors(trainSet, testInstance, 1)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have located the most similar neighbors for a test instance, the next task is to devise a predicted response based on those neighbors.\n",
    "\n",
    "We can do this by allowing each neighbor to vote for their class attribute, and take the majority vote as the prediction.\n",
    "\n",
    "Below provides a function for getting the majority voted response from a number of neighbors. It assumes the class is the last attribute for each neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def getResponse(neighbors):\n",
    "\tclassVotes = dict()\n",
    "\tfor x in range(len(neighbors)):\n",
    "\t\tresponse = neighbors[x][-1]\n",
    "\t\tif response in classVotes:\n",
    "\t\t\tclassVotes[response] += 1\n",
    "\t\telse:\n",
    "\t\t\tclassVotes[response] = 1\n",
    "\tsortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\treturn sortedVotes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "neighbors = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]\n",
    "response = getResponse(neighbors)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of the pieces of the kNN algorithm in place. An important remaining concern is how to evaluate the accuracy of predictions.\n",
    "\n",
    "An easy way to evaluate the accuracy of the model is to calculate a ratio of the total correct predictions out of all predictions made, called the classification accuracy.\n",
    "\n",
    "Below is the getAccuracy function that sums the total correct predictions and returns the accuracy as a percentage of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\tif testSet[x][-1] is predictions[x]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "testSet = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]\n",
    "predictions = ['a', 'a', 'a']\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 98\n",
      "Test set: 52\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-setosa', actual='Iris-setosa'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-virginica', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-versicolor', actual='Iris-versicolor'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-versicolor', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "> predicted='Iris-virginica', actual='Iris-virginica'\n",
      "Accuracy: 96.15384615384616%\n"
     ]
    }
   ],
   "source": [
    "# Example of kNN implemented from Scratch in Python\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "\n",
    "def loadDataset(filename, split, trainingSet=[] , testSet=[]):\n",
    "\twith open(filename, 'r') as csvfile:\n",
    "\t    lines = csv.reader(csvfile)\n",
    "\t    dataset = list(lines)\n",
    "\t    for x in range(len(dataset)-1):\n",
    "\t        for y in range(4):\n",
    "\t            dataset[x][y] = float(dataset[x][y])\n",
    "\t        if random.random() < split:\n",
    "\t            trainingSet.append(dataset[x])\n",
    "\t        else:\n",
    "\t            testSet.append(dataset[x])\n",
    "\n",
    "\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "\tdistance = 0\n",
    "\tfor x in range(length):\n",
    "\t\tdistance += pow((instance1[x] - instance2[x]), 2)\n",
    "\treturn math.sqrt(distance)\n",
    "\n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "\tdistances = []\n",
    "\tlength = len(testInstance)-1\n",
    "\tfor x in range(len(trainingSet)):\n",
    "\t\tdist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "\t\tdistances.append((trainingSet[x], dist))\n",
    "\tdistances.sort(key=operator.itemgetter(1))\n",
    "\tneighbors = []\n",
    "\tfor x in range(k):\n",
    "\t\tneighbors.append(distances[x][0])\n",
    "\treturn neighbors\n",
    "\n",
    "def getResponse(neighbors):\n",
    "\tclassVotes = {}\n",
    "\tfor x in range(len(neighbors)):\n",
    "\t\tresponse = neighbors[x][-1]\n",
    "\t\tif response in classVotes:\n",
    "\t\t\tclassVotes[response] += 1\n",
    "\t\telse:\n",
    "\t\t\tclassVotes[response] = 1\n",
    "\tsortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\treturn sortedVotes[0][0]\n",
    "\n",
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\tif testSet[x][-1] == predictions[x]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0\n",
    "\t\n",
    "def main():\n",
    "\t# prepare data\n",
    "\ttrainingSet=[]\n",
    "\ttestSet=[]\n",
    "\tsplit = 0.67\n",
    "\tloadDataset('iris.data', split, trainingSet, testSet)\n",
    "\tprint('Train set: ' + repr(len(trainingSet)))\n",
    "\tprint('Test set: ' + repr(len(testSet)))\n",
    "\t# generate predictions\n",
    "\tpredictions=[]\n",
    "\tk = 3\n",
    "\tfor x in range(len(testSet)):\n",
    "\t\tneighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "\t\tresult = getResponse(neighbors)\n",
    "\t\tpredictions.append(result)\n",
    "\t\tprint('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n",
    "\taccuracy = getAccuracy(testSet, predictions)\n",
    "\tprint('Accuracy: ' + repr(accuracy) + '%')\n",
    "\t\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas For Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides you with ideas for extensions that you could apply and investigate with the Python code you have implemented as part of this tutorial.\n",
    "\n",
    "Regression: You could adapt the implementation to work for regression problems (predicting a real-valued attribute). The summarization of the closest instances could involve taking the mean or the median of the predicted attribute.\n",
    "\n",
    "Normalization: When the units of measure differ between attributes, it is possible for attributes to dominate in their contribution to the distance measure. For these types of problems, you will want to rescale all data attributes into the range 0-1 (called normalization) before calculating similarity. Update the model to support data normalization.\n",
    "\n",
    "Alternative Distance Measure: There are many distance measures available, and you can even develop your own domain-specific distance measures if you like. Implement an alternative distance measure, such as Manhattan distance or the vector dot product.\n",
    "\n",
    "There are many more extensions to this algorithm you might like to explore. Two additional ideas include support for distance-weighted contribution for the k-most similar instances to the prediction and more advanced data tree-based structures for searching for similar instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
